<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>publications | Minhyeok  Lee</title>
    <meta name="author" content="Minhyeok  Lee">
    <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar.">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/icon.png">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://hydragon516.github.io/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Minhyeok </span>Lee</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">publications</h1>
            <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2025</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/DualGround.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/DualGround.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="kang2025empower" class="col-sm-6">
        <!-- Title -->
        <div class="title">[NeurIPS 2025] Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding</div>
        <!-- Author -->
        <div class="author">
        

        Minseok Kang, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Minjung Kim, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Donghyeong Kim, Sangyoun Lee' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>arXiv preprint arXiv:2510.20244</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2510.20244" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Video Temporal Grounding (VTG) aims to localize temporal segments in long, untrimmed videos that align with a given natural language query. This task typically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection (HD). While recent advances have been progressed by powerful pretrained vision-language models such as CLIP and InternVideo2, existing approaches commonly treat all text tokens uniformly during crossmodal attention, disregarding their distinct semantic roles. To validate the limitations of this approach, we conduct controlled experiments demonstrating that VTG models overly rely on [EOS]-driven global semantics while failing to effectively utilize word-level signals, which limits their ability to achieve fine-grained temporal alignment. Motivated by this limitation, we propose DualGround, a dual-branch architecture that explicitly separates global and local semantics by routing the [EOS] token through a sentence-level path and clustering word tokens into phrase-level units for localized grounding. Our method introduces (1) tokenrole-aware cross modal interaction strategies that align video features with sentence-level and phrase-level semantics in a structurally disentangled manner, and (2) a joint modeling framework that not only improves global sentence-level alignment but also enhances finegrained temporal grounding by leveraging structured phrase-aware context. This design allows the model to capture both coarse and localized semantics, enabling more expressive and context-aware video grounding. DualGround achieves state-of-the-art performance on both Moment Retrieval and Highlight Detection tasks across QVHighlights and CharadesSTA benchmarks, demonstrating the effectiveness of disentangled semantic modeling in video-language alignment.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/TMO++.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/TMO++.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="cho2025treating" class="col-sm-6">
        <!-- Title -->
        <div class="title">[TCSVT 2025] Treating Motion as Option with Output Selection for Unsupervised Video Object Segmentation</div>
        <!-- Author -->
        <div class="author">
        

        Suhwan Cho, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Jungho Lee, and
          <span class="more-authors" title="click to view 5 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '5 more authors' ? 'MyeongAh Cho, Seungwook Park, Jaeyeob Kim, Hyunsung Jang, Sangyoun Lee' : '5 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">5 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In </em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2309.14786" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/suhwan-cho/TMO" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Unsupervised video object segmentation aims to detect the most salient object in a video without any external guidance regarding the object. Salient objects often exhibit distinctive movements compared to the background, and recent methods leverage this by combining motion cues from optical flow maps with appearance cues from RGB images. However, because optical flow maps are often closely correlated with segmentation masks, networks can become overly dependent on motion cues during training, leading to vulnerability when faced with confusing motion cues and resulting in unstable predictions. To address this challenge, we propose a novel motion-as-option network that treats motion cues as an optional component rather than a necessity. During training, we randomly input RGB images into the motion encoder instead of optical flow maps, which implicitly reduces the network’s reliance on motion cues. This design ensures that the motion encoder is capable of processing both RGB images and optical flow maps, leading to two distinct predictions depending on the type of input provided. To make the most of this flexibility, we introduce an adaptive output selection algorithm that determines the optimal prediction during testing.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 동영상 (기본 미리보기) -->
                <video class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/Sparse-DeRF1.mp4" loop="" autoplay="" muted="" playsinline="">
                </video>

                
                    <!-- 내부 동영상 (호버 미리보기) -->
                    <video class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/Sparse-DeRF2.mp4" loop="" muted="" playsinline="" style="display:none;">
                    </video>
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2025sparse" class="col-sm-6">
        <!-- Title -->
        <div class="title">[TPAMI 2025] Sparse-DeRF: Deblurred Neural Radiance Fields from Sparse View</div>
        <!-- Author -->
        <div class="author">
        

        Dogyoon Lee, Donghyeong Kim, Jungho Lee, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Minhyeok Lee, Seunghoon Lee, Sangyoun Lee' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In </em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2407.06613" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://dogyoonlee.github.io/sparsederf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recent studies construct deblurred neural radiance fields (DeRF) using dozens of blurry images, which are not practical scenarios if only a limited number of blurry images are available. This paper focuses on constructing DeRF from sparse-view for more pragmatic real-world scenarios. As observed in our experiments, establishing DeRF from sparse views proves to be a more challenging problem due to the inherent complexity arising from the simultaneous optimization of blur kernels and NeRF from sparse view. Sparse-DeRF successfully regularizes the complicated joint optimization, presenting alleviated overfitting artifacts and enhanced quality on radiance fields. The regularization consists of three key components: Surface smoothness, helps the model accurately predict the scene structure utilizing unseen and additional hidden rays derived from the blur kernel based on statistical tendencies of real-world; Modulated gradient scaling, helps the model adjust the amount of the backpropagated gradient according to the arrangements of scene objects; Perceptual distillation improves the perceptual quality by overcoming the ill-posed multi-view inconsistency of image deblurring and distilling the pre-filtered information, compensating for the lack of clean information in blurry images. We demonstrate the effectiveness of the Sparse-DeRF with extensive quantitative and qualitative experimental results by training DeRF from 2-view, 4-view, and 6-view blurry images.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 동영상 (기본 미리보기) -->
                <video class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/FindTrack.mp4" loop="" autoplay="" muted="" playsinline="">
                </video>

                
                    <!-- 내부 동영상 (호버 미리보기) -->
                    <video class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/FindTrack.mp4" loop="" muted="" playsinline="" style="display:none;">
                    </video>
                  
                </div>

        <!-- Entry bib key -->
        <div id="cho2025find" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ICCVW 2025] Find First, Track Next: Decoupling Identification and Propagation in Referring Video Object Segmentation</div>
        <!-- Author -->
        <div class="author">
        

        Suhwan Cho*, Seunghoon Lee*, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Jungho Lee, Sangyoun Lee' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In </em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2503.03492" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/suhwan-cho/FindTrack" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Referring video object segmentation aims to segment and track a target object in a video using a natural language prompt. Existing methods typically fuse visual and textual features in a highly entangled manner, processing multi-modal information together to generate per-frame masks. However, this approach often struggles with ambiguous target identification, particularly in scenes with multiple similar objects, and fails to ensure consistent mask propagation across frames. To address these limitations, we introduce FindTrack, a novel decoupled framework that separates target identification from mask propagation. FindTrack first adaptively selects a key frame by balancing segmentation confidence and vision-text alignment, establishing a robust reference for the target object. This reference is then utilized by a dedicated propagation module to track and segment the object across the entire video. By decoupling these processes, FindTrack effectively reduces ambiguities in target association and enhances segmentation consistency. We demonstrate that FindTrack outperforms existing methods on public benchmarks.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/DepthFlow.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/DepthFlow.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="cho2025depthflow" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ICCVW 2025] DepthFlow: Exploiting Depth-Flow Structural Correlations for Unsupervised Video Object Segmentation</div>
        <!-- Author -->
        <div class="author">
        

        Suhwan Cho, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Jungho Lee, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Donghyeong Kim, Sangyoun Lee' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2507.19790" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/suhwan-cho/DepthFlow" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Unsupervised video object segmentation (VOS) aims to detect the most prominent object in a video. Recently, twostream approaches that leverage both RGB images and optical flow have gained significant attention, but their performance is fundamentally constrained by the scarcity of training data. To address this, we propose DepthFlow, a novel data generation method that synthesizes optical flow from single images. Our approach is driven by the key insight that VOS models depend more on structural information embedded in flow maps than on their geometric accuracy, and that this structure is highly correlated with depth. We first estimate a depth map from a source image and then convert it into a synthetic flow field that preserves essential structural cues. This process enables the transformation of large-scale image-mask pairs into image-flowmask training pairs, dramatically expanding the data available for network training. By training a simple encoderdecoder architecture with our synthesized data, we achieve new state-of-the-art performance on all public VOS benchmarks, demonstrating a scalable and effective solution to the data scarcity problem.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/TransFlow.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/TransFlow.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="cho2025transflow" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ICCVW 2025] TransFlow: Motion Knowledge Transfer from Video Diffusion Models to Video Salient Object Detection</div>
        <!-- Author -->
        <div class="author">
        

        Suhwan Cho, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Jungho Lee, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Sunghun Yang, Sangyoun Lee' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2507.19789" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/suhwan-cho/TransFlow" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Video salient object detection (SOD) relies on motion cues to distinguish salient objects from backgrounds, but training such models is limited by scarce video datasets compared to abundant image datasets. Existing approaches that use spatial transformations to create video sequences from static images fail for motion-guided tasks, as these transformations produce unrealistic optical flows that lack semantic understanding of motion. We present TransFlow, which transfers motion knowledge from pre-trained video diffusion models to generate realistic training data for video SOD. Video diffusion models have learned rich semantic motion priors from large-scale video data, understanding how different objects naturally move in real scenes. TransFlow leverages this knowledge to generate semantically-aware optical flows from static images, where objects exhibit natural motion patterns while preserving spatial boundaries and temporal coherence. Our method achieves improved performance across multiple benchmarks, demonstrating effective motion knowledge transfer.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 동영상 (기본 미리보기) -->
                <video class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/STATIC1.mp4" loop="" autoplay="" muted="" playsinline="">
                </video>

                
                    <!-- 내부 동영상 (호버 미리보기) -->
                    <video class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/STATIC2.mp4" loop="" muted="" playsinline="" style="display:none;">
                    </video>
                  
                </div>

        <!-- Entry bib key -->
        <div id="yang2025static" class="col-sm-6">
        <!-- Title -->
        <div class="title">STATIC: Surface Temporal Affine for TIme Consistency in Video Monocular Depth Estimation</div>
        <!-- Author -->
        <div class="author">
        

        Sunghun Yang, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Suhwan Cho, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Jungho Lee, Sangyoun Lee' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In </em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2412.01090" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Video monocular depth estimation is essential for applications such as autonomous driving, AR/VR, and robotics. Recent transformer-based single-image monocular depth estimation models perform well on single images but struggle with depth consistency across video frames. Traditional methods aim to improve temporal consistency using multi-frame temporal modules or prior information like optical flow and camera parameters. However, these approaches face issues such as high memory use, reduced performance with dynamic or irregular motion, and limited motion understanding. We propose STATIC, a novel model that independently learns temporal consistency in static and dynamic area without additional information. A difference mask from surface normals identifies static and dynamic area by measuring directional variance. For static area, the Masked Static (MS) module enhances temporal consistency by focusing on stable regions. For dynamic area, the Surface Normal Similarity (SNS) module aligns areas and enhances temporal consistency by measuring feature similarity between frames. A final refinement integrates the independently learned static and dynamic area, enabling STATIC to achieve temporal consistency across the entire sequence. Our method achieves state-of-the-art video depth estimation on the KITTI and NYUv2 datasets without additional information.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/ESC-Net1.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/ESC-Net2.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2025effective" class="col-sm-6">
        <!-- Title -->
        <div class="title">[CVPR 2025] Effective SAM Combination for Open-Vocabulary Semantic Segmentation - Oral (3.3% of the accepted papers)</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Suhwan Cho, Jungho Lee, and
          <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Sunghun Yang, Heeseung Choi, Ig-Jae Kim, Sangyoun Lee' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2411.14723" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Open-vocabulary semantic segmentation aims to assign pixel-level labels to images across an unlimited range of classes. Traditional methods address this by sequentially connecting a powerful mask proposal generator, such as the Segment Anything Model (SAM), with a pre-trained vision-language model like CLIP. But these two-stage approaches often suffer from high computational costs, memory inefficiencies. In this paper, we propose ESC-Net, a novel one-stage open-vocabulary segmentation model that leverages the SAM decoder blocks for class-agnostic segmentation within an efficient inference framework. By embedding pseudo prompts generated from image-text correlations into SAM’s promptable segmentation framework, ESC-Net achieves refined spatial aggregation for accurate mask predictions. ESC-Net achieves superior performance on standard benchmarks, including ADE20K, PASCAL-VOC, and PASCAL-Context, outperforming prior methods in both efficiency and accuracy. Comprehensive ablation studies further demonstrate its robustness across challenging conditions.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 동영상 (기본 미리보기) -->
                <video class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/coco1.mp4" loop="" autoplay="" muted="" playsinline="">
                </video>

                
                    <!-- 내부 동영상 (호버 미리보기) -->
                    <video class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/coco2.mp4" loop="" muted="" playsinline="" style="display:none;">
                    </video>
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2025cocoGaussian" class="col-sm-6">
        <!-- Title -->
        <div class="title">[CVPR 2025] CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images</div>
        <!-- Author -->
        <div class="author">
        

        Jungho Lee, Suhwan Cho, Taeoh Kim, and
          <span class="more-authors" title="click to view 6 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '6 more authors' ? 'Ho-Deok Jang, Minhyeok Lee, Geonho Cha, Dongyoon Wee, Dogyoon Lee, Sangyoun Lee' : '6 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">6 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2412.16028" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/Jho-Yonsei/CoCoGaussian" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>3D Gaussian Splatting (3DGS) has attracted significant attention for its high-quality novel view rendering, inspiring research to address real-world challenges. While conventional methods depend on sharp images for accurate scene reconstruction, real-world scenarios are often affected by defocus blur due to finite depth of field, making it essential to account for realistic 3D scene representation. In this study, we propose CoCoGaussian, a Circle of Confusion-aware Gaussian Splatting that enables precise 3D scene representation using only defocused images. CoCoGaussian addresses the challenge of defocus blur by modeling the Circle of Confusion (CoC) through a physically grounded approach based on the principles of photographic defocus. Exploiting 3D Gaussians, we compute the CoC diameter from depth and learnable aperture information, generating multiple Gaussians to precisely capture the CoC shape. Furthermore, we introduce a learnable scaling factor to enhance robustness and provide more flexibility in handling unreliable depth in scenes with reflective or refractive surfaces. Experiments on both synthetic and real-world datasets demonstrate that CoCoGaussian achieves state-of-the-art performance across multiple benchmarks.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/SMURF.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/SMURF.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2025smurf" class="col-sm-6">
        <!-- Title -->
        <div class="title">[CVPRW 2025] SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields</div>
        <!-- Author -->
        <div class="author">
        

        Jungho Lee, Dogyoon Lee, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Donghyung Kim, Sangyoun Lee' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In </em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2403.07547" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/Jho-Yonsei/SMURF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Neural radiance fields (NeRF) has attracted considerable attention for their exceptional ability in synthesizing novel views with high fidelity. However, the presence of motion blur, resulting from slight camera movements during extended shutter exposures, poses a significant challenge, potentially compromising the quality of the reconstructed 3D scenes. While recent studies have addressed this issue, they do not consider the continuous dynamics of camera movements during image acquisition, leading to inaccurate scene reconstruction. Additionally, these methods are plagued by slow training and rendering speed. To effectively handle these issues, we propose sequential motion understanding radiance fields (SMURF), a novel approach that employs neural ordinary differential equation (Neural-ODE) to model continuous camera motion and leverages the explicit volumetric representation method for faster training and robustness to motion-blurred input images. The core idea of the SMURF is continuous motion blurring kernel (CMBK), a unique module designed to model a continuous camera movements for processing blurry inputs. Our model, rigorously evaluated against benchmark datasets, demonstrates state-of-the-art performance both quantitatively and qualitatively.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 동영상 (기본 미리보기) -->
                <video class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/FFF_VDI1.mp4" loop="" autoplay="" muted="" playsinline="">
                </video>

                
                    <!-- 내부 동영상 (호버 미리보기) -->
                    <video class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/FFF_VDI2.mp4" loop="" muted="" playsinline="" style="display:none;">
                    </video>
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2024video" class="col-sm-6">
        <!-- Title -->
        <div class="title">[AAAI 2025] Video diffusion models are strong video inpainter</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Suhwan Cho, Chajin Shin, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Jungho Lee, Sunghun Yang, Sangyoun Lee' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>arXiv preprint arXiv:2408.11402</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2408.11402" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/Hydragon516/FFF-VDI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Propagation-based video inpainting using optical flow at the pixel or feature level has recently garnered significant attention. However, it has limitations such as the inaccuracy of optical flow prediction and the propagation of noise over time. These issues result in non-uniform noise and time consistency problems throughout the video, which are particularly pronounced when the removed area is large and involves substantial movement. To address these issues, we propose a novel First Frame Filling Video Diffusion Inpainting model (FFF-VDI). We design FFF-VDI inspired by the capabilities of pre-trained image-to-video diffusion models that can transform the first frame image into a highly natural video. To apply this to the video inpainting task, we propagate the noise latent information of future frames to fill the masked areas of the first frame’s noise latent code. Next, we fine-tune the pre-trained image-to-video diffusion model to generate the inpainted video. The proposed model addresses the limitations of existing methods that rely on optical flow quality, producing much more natural and temporally consistent videos. This proposed approach is the first to effectively integrate image-to-video diffusion models into video inpainting tasks. Through various comparative experiments, we demonstrate that the proposed model can robustly handle diverse inpainting types with high quality.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/CRiM-GS.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/CRiM-GS.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2025como" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ICCV 2025] CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from Motion-Blurred Images</div>
        <!-- Author -->
        <div class="author">
        

        Jungho Lee, Donghyeong Kim, Dogyoon Lee, and
          <span class="more-authors" title="click to view 6 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '6 more authors' ? 'Suhwan Cho, Minhyeok Lee, Wonjoon Lee, Taeoh Kim, Dongyoon Wee, Sangyoun Lee' : '6 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">6 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>arXiv preprint arXiv:2503.05332</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2503.05332" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/Jho-Yonsei/CoMoGaussian" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>3D Gaussian Splatting (3DGS) has gained significant attention for their high-quality novel view rendering, motivating research to address real-world challenges. A critical issue is the camera motion blur caused by movement during exposure, which hinders accurate 3D scene reconstruction. In this study, we propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that reconstructs precise 3D scenes from motion-blurred images while maintaining real-time rendering speed. Considering the complex motion patterns inherent in real-world camera movements, we predict continuous camera trajectories using neural ordinary differential equations (ODEs). To ensure accurate modeling, we employ rigid body transformations, preserving the shape and size of the object but rely on the discrete integration of sampled frames. To better approximate the continuous nature of motion blur, we introduce a continuous motion refinement (CMR) transformation that refines rigid transformations by incorporating additional learnable parameters. By revisiting fundamental camera theory and leveraging advanced neural ODE techniques, we achieve precise modeling of continuous camera trajectories, leading to improved reconstruction accuracy. Extensive experiments demonstrate state-of-the-art performance both quantitatively and qualitatively on benchmark datasets, which include a wide range of motion blur scenarios, from moderate to extreme blur.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2024</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/GSA-Net.gif">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/GSA-Net.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2024guided" class="col-sm-6">
        <!-- Title -->
        <div class="title">[CVPR 2024] Guided Slot Attention for Unsupervised Video Object Segmentation</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Suhwan Cho, Dogyoon Lee, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Chaewon Park, Jungho Lee, Sangyoun Lee' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2303.08314" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/Hydragon516/GSANet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Unsupervised video object segmentation aims to segment the most prominent object in a video sequence. However, the existence of complex backgrounds and multiple foreground objects make this task challenging. To address this issue, we propose a guided slot attention network to reinforce spatial structural information and obtain better foreground–background separation. The foreground and background slots, which are initialized with query guidance, are iteratively refined based on interactions with template information. Furthermore, to improve slot–template interaction and effectively fuse global and local features in the target and reference frames, K-nearest neighbors filtering and a feature aggregation transformer are introduced. The proposed model achieves state-of-the-art performance on two popular datasets. Additionally, we demonstrate the robustness of the proposed model in challenging scenes through various comparative experiments.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/DPA.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/DPA.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="cho2024dual" class="col-sm-6">
        <!-- Title -->
        <div class="title">[CVPR 2024] Dual prototype attention for unsupervised video object segmentation</div>
        <!-- Author -->
        <div class="author">
        

        Suhwan Cho, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Seunghoon Lee, and
          <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Dogyoon Lee, Heeseung Choi, Ig-Jae Kim, Sangyoun Lee' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2211.12036" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/Hydragon516/DPA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Unsupervised video object segmentation (VOS) aims to detect and segment the most salient object in videos. The primary techniques used in unsupervised VOS are 1) the collaboration of appearance and motion information and 2) temporal fusion between different frames. This paper proposes two novel prototype-based attention mechanisms, inter-modality attention (IMA) and inter-frame attention (IFA), to incorporate these techniques via dense propagation across different modalities and frames. IMA densely integrates context information from different modalities based on a mutual refinement. IFA injects global context of a video to the query frame, enabling a full utilization of useful properties from multiple frames. Experimental results on public benchmark datasets demonstrate that our proposed approach outperforms all existing methods by a substantial margin. The proposed two components are also thoroughly validated via ablative study.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/BTMAE.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/BTMAE.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2023guided" class="col-sm-6">
        <!-- Title -->
        <div class="title">Synchronizing Vision and Language: Bidirectional Token-Masking AutoEncoder for Referring Image Segmentation</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Dogyoon Lee, Jungho Lee, and
          <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Suhwan Cho, Heeseung Choi, Ig-Jae Kim, Sangyoun Lee' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>arXiv preprint arXiv:2311.17952</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2311.17952" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Referring Image Segmentation (RIS) aims to segment target objects expressed in natural language within a scene at the pixel level. Various recent RIS models have achieved state-of-the-art performance by generating contextual tokens to model multimodal features from pretrained encoders and effectively fusing them using transformer-based cross-modal attention. While these methods match language features with image features to effectively identify likely target objects, they often struggle to correctly understand contextual information in complex and ambiguous sentences and scenes. To address this issue, we propose a novel bidirectional token-masking autoencoder (BTMAE) inspired by the masked autoencoder (MAE). The proposed model learns the context of image-to-language and language-to-image by reconstructing missing features in both image and language features at the token level. In other words, this approach involves mutually complementing across the features of images and language, with a focus on enabling the network to understand interconnected deep contextual information between the two modalities. This learning method enhances the robustness of RIS performance in complex sentences and scenes. Our BTMAE achieves state-of-the-art performance on three popular datasets, and we demonstrate the effectiveness of the proposed method through various ablation studies.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/PMN.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/PMN.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2023unsupervised" class="col-sm-6">
        <!-- Title -->
        <div class="title">[WACV 2023] Unsupervised Video Object Segmentation via Prototype Memory Network</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Suhwan Cho, Seunghoon Lee, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Chaewon Park, Sangyoun Lee' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2209.03712" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/Hydragon516/PMN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Unsupervised video object segmentation aims to segment a target object in the video without a ground truth mask in the initial frame. This challenging task requires extracting features for the most salient common objects within a video sequence. This difficulty can be solved by using motion information such as optical flow, but using only the information between adjacent frames results in poor connectivity between distant frames and poor performance. To solve this problem, we propose a novel prototype memory network architecture. The proposed model effectively extracts the RGB and motion information by extracting superpixel-based component prototypes from the input RGB images and optical flow maps. In addition, the model scores the usefulness of the component prototypes in each frame based on a self-learning algorithm and adaptively stores the most useful prototypes in memory and discards obsolete prototypes. We use the prototypes in the memory bank to predict the next query frames mask, which enhances the association between distant frames to help with accurate mask prediction. Our method is evaluated on three datasets, achieving state-of-the-art performance. We prove the effectiveness of the proposed model with various ablation studies.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/adagcn.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/adagcn.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2023adaptive" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ICIP 2023] Adaptive Graph Convolution Module for Salient Object Detection</div>
        <!-- Author -->
        <div class="author">
        

        Yongwoo Lee, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Suhwan Cho, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Sangyoun Lee' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>2023 IEEE International Conference on Image Processing</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2303.09801" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Salient object detection (SOD) is a task that involves identifying and segmenting the most visually prominent object in an image. Existing solutions can accomplish this use a multi-scale feature fusion mechanism to detect the global context of an image. However, as there is no consideration of the structures in the image nor the relations between distant pixels, conventional methods cannot deal with complex scenes effectively. In this paper, we propose an adaptive graph convolution module (AGCM) to overcome these limitations. Prototype features are initially extracted from the input image using a learnable region generation layer that spatially groups features in the image. The prototype features are then refined by propagating information between them based on a graph architecture, where each feature is regarded as a node. Experimental results show that the proposed AGCM dramatically improves the SOD performance both quantitatively and quantitatively.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/HD-GCN.gif">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/HD-GCN.gif" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2023hierarchically" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ICCV 2023] Hierarchically decomposed graph convolutional networks for skeleton-based action recognition</div>
        <!-- Author -->
        <div class="author">
        

        Jungho Lee, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Dogyoon Lee, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Sangyoun Lee' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2208.10741" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/Jho-Yonsei/HD-GCN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Graph convolutional networks (GCNs) are the most commonly used methods for skeleton-based action recognition and have achieved remarkable performance. Generating adjacency matrices with semantically meaningful edges is particularly important for this task, but extracting such edges is challenging problem. To solve this, we propose a hierarchically decomposed graph convolutional network (HD-GCN) architecture with a novel hierarchically decomposed graph (HD-Graph). The proposed HD-GCN effectively decomposes every joint node into several sets to extract major structurally adjacent and distant edges, and uses them to construct an HD-Graph containing those edges in the same semantic spaces of a human skeleton. In addition, we introduce an attention-guided hierarchy aggregation (A-HA) module to highlight the dominant hierarchical edge sets of the HD-Graph. Furthermore, we apply a new six-way ensemble method, which uses only joint and bone stream without any motion stream. The proposed model is evaluated and achieves state-of-the-art performance on three large, popular datasets: NTU-RGB+D 60, NTU-RGB+D 120, and Northwestern-UCLA. Finally, we demonstrate the effectiveness of our model with various comparative experiments.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/TMO.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/TMO.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="cho2023treating" class="col-sm-6">
        <!-- Title -->
        <div class="title">[WACV 2023] Treating motion as option to reduce motion dependency in unsupervised video object segmentation</div>
        <!-- Author -->
        <div class="author">
        

        Suhwan Cho, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Seunghoon Lee, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Chaewon Park, Donghyeong Kim, Sangyoun Lee' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2209.03138" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/suhwan-cho/tmo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Unsupervised video object segmentation (VOS) aims to detect the most salient object in a video sequence at the pixel level. In unsupervised VOS, most state-of-the-art methods leverage motion cues obtained from optical flow maps in addition to appearance cues to exploit the property that salient objects usually have distinctive movements compared to the background. However, as they are overly dependent on motion cues, which may be unreliable in some cases, they cannot achieve stable prediction. To reduce this motion dependency of existing two-stream VOS methods, we propose a novel motion-as-option network that optionally utilizes motion cues. Additionally, to fully exploit the property of the proposed network that motion is not always required, we introduce a collaborative network learning strategy. On all the public benchmark datasets, our proposed network affords state-of-the-art performance with real-time inference speed.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/TSANET.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/TSANET.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2023tsanet" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ICIP 2023] TSANET: Temporal and Scale Alignment for Unsupervised Video Object Segmentation</div>
        <!-- Author -->
        <div class="author">
        

        Seunghoon Lee, Suhwan Cho, Dogyoon Lee, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Minhyeok Lee, Sangyoun Lee' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>2023 IEEE International Conference on Image Processing</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2303.04376" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Unsupervised Video Object Segmentation (UVOS) refers to the challenging task of segmenting the prominent object in videos without manual guidance. In other words, the network detects the accurate region of the target object in a sequence of RGB frames without prior knowledge. In recent works, two approaches for UVOS have been discussed that can be divided into: appearance and appearance-motion based methods. Appearance based methods utilize the correlation information of inter-frames to capture target object that commonly appears in a sequence. However, these methods does not consider the motion of target object due to exploit the correlation information between randomly paired frames. Appearance-motion based methods, on the other hand, fuse the appearance features from RGB frames with the motion features from optical flow. Motion cue provides useful information since salient objects typically show distinctive motion in a sequence. However, these approaches have the limitation that the dependency on optical flow is dominant. In this paper, we propose a novel framework for UVOS that can address aforementioned limitations of two approaches in terms of both time and scale. Temporal Alignment Fusion aligns the saliency information of adjacent frames with the target frame to leverage the information of adjacent frames. Scale Alignment Decoder predicts the target object mask precisely by aggregating differently scaled feature maps via continuous mapping with implicit neural representation. We present experimental results on public benchmark datasets, DAVIS 2016 and FBMS, which demonstrate the effectiveness of our method. Furthermore, we outperform the state-of-the-art methods on DAVIS 2016.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/TSDN.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/TSDN.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="park2023two" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ICASSP 2023] Two-stream Decoder Feature Normality Estimating Network for Industrial Anomaly Detection</div>
        <!-- Author -->
        <div class="author">
        

        Chaewon Park, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Suhwan Cho, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Donghyeong Kim, Sangyoun Lee' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE International Conference on Acoustics, Speech and Signal Processing 2023</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2302.09794" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Image reconstruction-based anomaly detection has recently been in the spotlight because of the difficulty of constructing anomaly datasets. These approaches work by learning to model normal features without seeing abnormal samples during training and then discriminating anomalies at test time based on the reconstructive errors. However, these models have limitations in reconstructing the abnormal samples due to their indiscriminate conveyance of features. Moreover, these approaches are not explicitly optimized for distinguishable anomalies. To address these problems, we propose a two-stream decoder network (TSDN), designed to learn both normal and abnormal features. Additionally, we propose a feature normality estimator (FNE) to eliminate abnormal features and prevent high-quality reconstruction of abnormal regions. Evaluation on a standard benchmark demonstrated performance better than state-of-the-art models.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/STCNet.gif">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/STCNet.gif" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2023leveraging" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ICCV 2023] Leveraging spatio-temporal dependency for skeleton-based action recognition</div>
        <!-- Author -->
        <div class="author">
        

        Jungho Lee, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Suhwan Cho, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Sungmin Woo, Sungjun Jang, Sangyoun Lee' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2212.04761" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Skeleton-based action recognition has attracted considerable attention due to its compact skeletal structure of the human body. Many recent methods have achieved remarkable performance using graph convolutional networks (GCNs) and convolutional neural networks (CNNs), which extract spatial and temporal features, respectively. Although spatial and temporal dependencies in the human skeleton have been explored, spatio-temporal dependency is rarely considered. In this paper, we propose the Inter-Frame Curve Network (IFC-Net) to effectively leverage the spatio-temporal dependency of the human skeleton. Our proposed network consists of two novel elements: 1) The Inter-Frame Curve (IFC) module; and 2) Dilated Graph Convolution (D-GC). The IFC module increases the spatio-temporal receptive field by identifying meaningful node connections between every adjacent frame and generating spatio-temporal curves based on the identified node connections. The D-GC allows the network to have a large spatial receptive field, which specifically focuses on the spatial domain. The kernels of D-GC are computed from the given adjacency matrices of the graph and reflect large receptive field in a way similar to the dilated CNNs. Our IFC-Net combines these two modules and achieves state-of-the-art performance on three skeleton-based action recognition benchmarks: NTU-RGB+D 60, NTU-RGB+D 120, and Northwestern-UCLA.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/DP-Nerf.gif">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/DP-Nerf.gif" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="Lee_2023_CVPR" class="col-sm-6">
        <!-- Title -->
        <div class="title">[CVPR 2023] DP-NeRF: Deblurred Neural Radiance Field With Physical Scene Priors</div>
        <!-- Author -->
        <div class="author">
        

        Dogyoon Lee, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Chajin Shin, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Sangyoun Lee' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2211.12046" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/dogyoonlee/DP-NeRF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Neural Radiance Field (NeRF) has exhibited outstanding three-dimensional (3D) reconstruction quality via the novel view synthesis from multi-view images and paired calibrated camera parameters. However, previous NeRF-based systems have been demonstrated under strictly controlled settings, with little attention paid to less ideal scenarios, including with the presence of noise such as exposure, illumination changes, and blur. In particular, though blur frequently occurs in real situations, NeRF that can handle blurred images has received little attention. The few studies that have investigated NeRF for blurred images have not considered geometric and appearance consistency in 3D space, which is one of the most important factors in 3D reconstruction. This leads to inconsistency and the degradation of the perceptual quality of the constructed scene. Hence, this paper proposes a DP-NeRF, a novel clean NeRF framework for blurred images, which is constrained with two physical priors. These priors are derived from the actual blurring process during image acquisition by the camera. DP-NeRF proposes rigid blurring kernel to impose 3D consistency utilizing the physical priors and adaptive weight proposal to refine the color composition error in consideration of the relationship between depth and blur. We present extensive experimental results for synthetic and real scenes with two types of blur: camera motion blur and defocus blur. The results demonstrate that DP-NeRF successfully improves the perceptual quality of the constructed NeRF ensuring 3D geometric and appearance consistency. We further demonstrate the effectiveness of our model with comprehensive ablation analysis.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/ESA.gif">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/ESA.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2022robust" class="col-sm-6">
        <!-- Title -->
        <div class="title">[WACV 2022] Robust lane detection via expanded self attention</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Junhyeop Lee, Dogyoon Lee, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Woojin Kim, Sangwon Hwang, Sangyoun Lee' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2102.07037" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/Hydragon516/ESA-official" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The image-based lane detection algorithm is one of the key technologies in autonomous vehicles. Modern deep learning methods achieve high performance in lane detection, but it is still difficult to accurately detect lanes in challenging situations such as congested roads and extreme lighting conditions. To be robust on these challenging situations, it is important to extract global contextual information even from limited visual cues. In this paper, we propose a simple but powerful self-attention mechanism optimized for lane detection called the Expanded Self Attention (ESA) module. Inspired by the simple geometric structure of lanes, the proposed method predicts the confidence of a lane along the vertical and horizontal directions in an image. The prediction of the confidence enables estimating occluded locations by extracting global contextual information. ESA module can be easily implemented and applied to any encoder-decoder-based model without increasing the inference time. The performance of our method is evaluated on three popular lane detection benchmarks (TuSimple, CULane and BDD100K). We achieve state-of-the-art performance in CULane and BDD100K and distinct improvement on TuSimple dataset. The experimental results show that our approach is robust to occlusion and extreme lighting conditions.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/TBD.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/TBD.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="cho2022tackling" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ECCV 2022] Tackling background distraction in video object segmentation</div>
        <!-- Author -->
        <div class="author">
        

        Suhwan Cho, Heansung Lee, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, and
          <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Chaewon Park, Sungjun Jang, Minjung Kim, Sangyoun Lee' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXII</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2207.06953" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/suhwan-cho/TBD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Semi-supervised video object segmentation (VOS) aims to densely track certain designated objects in videos. One of the main challenges in this task is the existence of background distractors that appear similar to the target objects. We propose three novel strategies to suppress such distractors: 1) a spatio-temporally diversified template construction scheme to obtain generalized properties of the target objects; 2) a learnable distance-scoring function to exclude spatially-distant distractors by exploiting the temporal consistency between two consecutive frames; 3) swap-and-attach augmentation to force each object to have unique features by providing training samples containing entangled objects. On all public benchmark datasets, our model achieves a comparable performance to contemporary state-of-the-art approaches, even with real-time performance. Qualitative results also demonstrate the superiority of our approach over existing methods. We believe our approach will be widely used for future VOS research.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/CFDN.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/CFDN.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="park2022saliency" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ICIP 2022] Saliency detection via global context enhanced feature fusion and edge weighted loss</div>
        <!-- Author -->
        <div class="author">
        

        Chaewon Park, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, MyeongAh Cho, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Sangyoun Lee' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In 2022 IEEE International Conference on Image Processing</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2110.06550" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>UNet-based methods have shown outstanding performance in salient object detection (SOD), but are problematic in two aspects. 1) Indiscriminately integrating the encoder feature, which contains spatial information for multiple objects, and the decoder feature, which contains global information of the salient object, is likely to convey unnecessary details of non-salient objects to the decoder, hindering saliency detection. 2) To deal with ambiguous object boundaries and generate accurate saliency maps, the model needs additional branches, such as edge reconstructions, which leads to increasing computational cost. To address the problems, we propose a context fusion decoder network (CFDN) and near edge weighted loss (NEWLoss) function. The CFDN creates an accurate saliency map by integrating global context information and thus suppressing the influence of the unnecessary spatial information. NEWLoss accelerates learning of obscure boundaries without additional modules by generating weight maps on object boundaries. Our method is evaluated on four benchmarks and achieves state-of-the-art performance. We prove the effectiveness of the proposed method through comparative experiments.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/SGCN.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/SGCN.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2022superpixel" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ICIP 2022] Superpixel Group-Correlation Network for Co-Saliency Detection</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Chaewon Park, Suhwan Cho, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Sangyoun Lee' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In 2022 IEEE International Conference on Image Processing</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Co-saliency detection is a task to segment the occurring salient objects in a group of images. The biggest challenges are distracting objects in the background and ambiguity between the foreground and background. To handle these issues, we propose a novel superpixel group-correlation network (SGCN) architecture that uses a superpixel algorithm to obtain various component features from a group of images and creates a group-correlation matrix to detect the common components of those images. In this way, non-common objects can be effectively excluded from consideration, enabling a clear distinction between foreground and background. Our method outperforms current state-of-the-art methods on three popular benchmark datasets for co-saliency detection, and our extensive experiments thoroughly validate our claimed contributions.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/RandomSEMO.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/RandomSEMO.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="park2022randomsemo" class="col-sm-6">
        <!-- Title -->
        <div class="title">RandomSEMO: Normality Learning Of Moving Objects For Video Anomaly Detection</div>
        <!-- Author -->
        <div class="author">
        

        Chaewon Park, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, MyeongAh Cho, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Sangyoun Lee' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>arXiv preprint arXiv:2202.06256</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2202.06256" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recent anomaly detection algorithms have shown powerful performance by adopting frame predicting autoencoders. However, these methods face two challenging circumstances. First, they are likely to be trained to be excessively powerful, generating even abnormal frames well, which leads to failure in detecting anomalies. Second, they are distracted by the large number of objects captured in both foreground and background. To solve these problems, we propose a novel superpixel-based video data transformation technique named Random Superpixel Erasing on Moving Objects (RandomSEMO) and Moving Object Loss (MOLoss), built on top of a simple lightweight autoencoder. RandomSEMO is applied to the moving object regions by randomly erasing their superpixels. It enforces the network to pay attention to the foreground objects and learn the normal features more effectively, rather than simply predicting the future frame. Moreover, MOLoss urges the model to focus on learning normal objects captured within RandomSEMO by amplifying the loss on the pixels near the moving objects. The experimental results show that our model outperforms state-of-the-arts on three benchmarks.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/SPSN.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/SPSN.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2022spsn" class="col-sm-6">
        <!-- Title -->
        <div class="title">[ECCV 2022] Spsn: Superpixel prototype sampling network for rgb-d salient object detection</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee*</a>, Chaewon Park*, Suhwan Cho, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Sangyoun Lee' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIX</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2207.07898" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/Hydragon516/SPSN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>RGB-D salient object detection (SOD) has been in the spotlight recently because it is an important preprocessing operation for various vision tasks. However, despite advances in deep learning-based methods, RGB-D SOD is still challenging due to the large domain gap between an RGB image and the depth map and low-quality depth maps. To solve this problem, we propose a novel superpixel prototype sampling network (SPSN) architecture. The proposed model splits the input RGB image and depth map into component superpixels to generate component prototypes. We design a prototype sampling network so that the network only samples prototypes corresponding to salient objects. In addition, we propose a reliance selection module to recognize the quality of each RGB and depth feature map and adaptively weight them in proportion to their reliability. The proposed method makes the model robust to inconsistencies between RGB images and depth maps and eliminates the influence of non-salient objects. Our method is evaluated on five popular datasets, achieving state-of-the-art performance. We prove the effectiveness of the proposed method through comparative experiments.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/FastAno.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/FastAno.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="park2022fastano" class="col-sm-6">
        <!-- Title -->
        <div class="title">[WACV 2022] FastAno: Fast anomaly detection via spatio-temporal patch transformation</div>
        <!-- Author -->
        <div class="author">
        

        Chaewon Park, MyeongAh Cho, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Sangyoun Lee' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2106.08613" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Video anomaly detection has gained significant attention due to the increasing requirements of automatic monitoring for surveillance videos. Especially, the prediction based approach is one of the most studied methods to detect anomalies by predicting frames that include abnormal events in the test set after learning with the normal frames of the training set. However, a lot of prediction networks are computationally expensive owing to the use of pre-trained optical flow networks, or fail to detect abnormal situations because of their strong generative ability to predict even the anomalies. To address these shortcomings, we propose spatial rotation transformation (SRT) and temporal mixing transformation (TMT) to generate irregular patch cuboids within normal frame cuboids in order to enhance the learning of normal features. Additionally, the proposed patch transformation is used only during the training phase, allowing our model to detect abnormal frames at fast speed during inference. Our model is evaluated on three anomaly detection benchmarks, achieving competitive accuracy and surpassing all the previous works in terms of speed.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/Edgeconv.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/Edgeconv.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2022edgeconv" class="col-sm-6">
        <!-- Title -->
        <div class="title">[WACV 2022] Edgeconv with attention module for monocular depth estimation</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Sangwon Hwang, Chaewon Park, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Sangyoun Lee' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2106.08615" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Monocular depth estimation is an especially important task in robotics and autonomous driving, where 3D structural information is essential. However, extreme lighting conditions and complex surface objects make it difficult to predict depth in a single image. Therefore, to generate accurate depth maps, it is important for the model to learn structural information about the scene. We propose a novel Patch-Wise EdgeConv Module (PEM) and EdgeConv Attention Module (EAM) to solve the difficulty of monocular depth estimation. The proposed modules extract structural information by learning the relationship between image patches close to each other in space using edge convolution. Our method is evaluated on two popular datasets, the NYU Depth V2 and the KITTI Eigen split, achieving state-of-the-art performance. We prove that the proposed model predicts depth robustly in challenging scenes through various comparative experiments.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/DPS-Net.gif">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/DPS-Net.gif" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2022global" class="col-sm-6">
        <!-- Title -->
        <div class="title">Boundary-aware Camouflaged Object Detection via Deformable Point Sampling</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, Suhwan Cho, Chaewon Park, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Dogyoon Lee, Jungho Lee, Sangyoun Lee' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>arXiv preprint arXiv:2211.12048</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2211.12048" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The camouflaged object detection (COD) task aims to identify and segment objects that blend into the background due to their similar color or texture. Despite the inherent difficulties of the task, COD has gained considerable attention in several fields, such as medicine, life-saving, and anti-military fields. In this paper, we propose a novel solution called the Deformable Point Sampling network (DPS-Net) to address the challenges associated with COD. The proposed DPS-Net utilizes a Deformable Point Sampling transformer (DPS transformer) that can effectively capture sparse local boundary information of significant object boundaries in COD using a deformable point sampling method. Moreover, the DPS transformer demonstrates robust COD performance by extracting contextual features for target object localization through integrating rough global positional information of objects with boundary local information. We evaluate our method on three prominent datasets and achieve state-of-the-art performance. Our results demonstrate the effectiveness of the proposed method through comparative experiments.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/EMVOS.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/EMVOS.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="cho2022pixel" class="col-sm-6">
        <!-- Title -->
        <div class="title">Pixel-Level Equalized Matching for Video Object Segmentation</div>
        <!-- Author -->
        <div class="author">
        

        Suhwan Cho, Woo Jin Kim, MyeongAh Cho, and
          <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Seunghoon Lee, Minhyeok Lee, Chaewon Park, Sangyoun Lee' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>arXiv preprint arXiv:2209.03139</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2209.03139" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Feature similarity matching, which transfers the information of the reference frame to the query frame, is a key component in semi-supervised video object segmentation. If surjective matching is adopted, background distractors can easily occur and degrade the performance. Bijective matching mechanisms try to prevent this by restricting the amount of information being transferred to the query frame, but have two limitations: 1) surjective matching cannot be fully leveraged as it is transformed to bijective matching at test time; and 2) test-time manual tuning is required for searching the optimal hyper-parameters. To overcome these limitations while ensuring reliable information transfer, we introduce an equalized matching mechanism. To prevent the reference frame information from being overly referenced, the potential contribution to the query frame is equalized by simply applying a softmax operation along with the query. On public benchmark datasets, our proposed approach achieves a comparable performance to state-of-the-art methods.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/rsmix.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/rsmix.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2021regularization" class="col-sm-6">
        <!-- Title -->
        <div class="title">[CVPR 2021] Regularization strategy for point cloud via rigidly mixed sample</div>
        <!-- Author -->
        <div class="author">
        

        Dogyoon Lee, Jaeha Lee, Junhyeop Lee, and
          <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Hyeongmin Lee, Minhyeok Lee, Sungmin Woo, Sangyoun Lee' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2102.01929" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://github.com/dogyoonlee/RSMix" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Data augmentation is an effective regularization strategy to alleviate the overfitting, which is an inherent drawback of the deep neural networks. However, data augmentation is rarely considered for point cloud processing despite many studies proposing various augmentation methods for image data. Actually, regularization is essential for point clouds since lack of generality is more likely to occur in point cloud due to small datasets. This paper proposes a Rigid Subset Mix (RSMix), a novel data augmentation method for point clouds that generates a virtual mixed sample by replacing part of the sample with shape-preserved subsets from another sample. RSMix preserves structural information of the point cloud sample by extracting subsets from each sample without deformation using a neighboring function. The neighboring function was carefully designed considering unique properties of point cloud, unordered structure and non-grid. Experiments verified that RSMix successfully regularized the deep neural networks with remarkable improvement for shape classification. We also analyzed various combinations of data augmentations including RSMix with single and multi-view evaluations, based on abundant ablation studies.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-8 preview" onmouseover="
       /* 
         1) default-item(기본 미리보기), hover-item(호버 미리보기) 찾기 
         2) 둘 다 video 태그이면(동영상이라면) 시간을 맞추고 hover 영상 재생
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (defItem &amp;&amp; hovItem 
           &amp;&amp; defItem.tagName.toLowerCase() === 'video'
           &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.currentTime = defItem.currentTime;
         hovItem.play();
       }
       /* 기본 미리보기 숨기고, 호버 미리보기 노출 */
       if(hovItem) hovItem.style.display='block';
       if(defItem) defItem.style.display='none';
     " onmouseout="
       /* 
         1) hover-item(호버 미리보기) 정지 
         2) 기본 미리보기 다시 노출
       */
       var defItem = this.querySelector('.default-item');
       var hovItem = this.querySelector('.hover-item');
       if (hovItem &amp;&amp; hovItem.tagName.toLowerCase() === 'video') {
         hovItem.pause();
       }
       if(hovItem) hovItem.style.display='none';
       if(defItem) defItem.style.display='block';
     ">
<!-- 내부 이미지 (기본 미리보기) -->
                <img class="default-item preview z-depth-1 rounded" src="/assets/img/publication_preview/MFMA.png">

                
                    <!-- 내부 이미지 (호버) -->
                    <img class="hover-item preview z-depth-1 rounded" src="/assets/img/publication_preview/MFMA.png" style="display:none;">
                  
                </div>

        <!-- Entry bib key -->
        <div id="lee2021multi" class="col-sm-6">
        <!-- Title -->
        <div class="title">Multi-level Feature Maps Attention for Monocular Depth Estimation</div>
        <!-- Author -->
        <div class="author">
        

        Seunghoon Lee, <a href="https://scholar.google.co.kr/citations?user=WGchT7cAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Minhyeok Lee</a>, and Sangyoon Lee</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In 2021 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)</em>, 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>[ICCE-Asia 2021] Monocular depth estimation is a fundamental task in autonomous driving, robotics, virtual reality. Monocular depth estimation is attracting research due to the efficiency of predicting depth map from a single RGB image. However, Monocular depth estimation is an ill-posed problem and is sensitive to image compositions such as light condition, occlusion, noise. We propose an encoder-decoder based network that uses multi-level attention and aggregate densely weighted feature map. Our model is evaluated on NYU Depth v2. Experimental results demonstrated that our model achieves promising performance.</p>
          </div>
        </div>
      </div>
</li>
</ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Minhyeok  Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
